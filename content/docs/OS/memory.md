# Memory management

## Memory hiarrarchy
![image info](../memory_management.png)

![pic](https://gabrieletolomei.files.wordpress.com/2013/10/program_in_memory2.png)

## Address spaces

Exposing physical meory to processes has several major drawbacks:
1. use can address every byte of memory so they can trash the OS, intentionally or by accident.
2. difficult to have multiple programs running at once.

A better solution is to invent a new abstraction for memory: *the address space*. An address space is the set of addresses that a process can use to address memory. Each process has its own address space, independent of those belonging to other processes (except in some special circumstances where processes want to share their address spaces).
### base and limit registers

Every time a process references memory, either to fetch an instruction or read or write a data word, the CPU hardware automatically adds the base value to the address generated by the process before sending the address out on the memory bus. Simultaneously, it checks whether the address offered is equal to or greater than the value in the limit register, in which case a fault is generated and the access is aborted.

In many implementations, the base and limit registers are protected in such a way that only the operating system can modify them. 

A disadvantage of relocation using base and limit registers is the need to per- form an addition and a comparison on every memory reference. Comparisons can be done fast, but additions are slow due to carry-propagation time unless special addition circuits are used.

## managing free memory
When memory is assigned dynamically, the operating system must manage it. In general terms, there are two ways to keep track of memory usage: bitmaps and free lists.

![pic](../bitmaps.png)

### bitmaps

### free lists

---
## Swapping

{{<hint info>}}
**内存不够怎么办?
1. swapping
2. virtual memory (covered in next section )**
{{</hint>}}

swapping
: Bringing in each process in its entirety, running it for a while, then putting it back on the disk. Idle processes are mostly stored on disk, so they do not take up any memory when they are not running (although some of them wake up periodically to do their work, then go to sleep again). 

![swapping](../swapping.png)

When swapping creates multiple holes in memory, it is possible to combine them all into one big one by moving all the processes downward as far as possible. This technique is known as memory compaction. It is usually not done because it requires a lot of CPU time. 

fixed allocated memory sounds easy, but if processes' data segment and stack can grow, then it's a good idea to allocate extra memory so they have room for growth.

## Virtual Memory
virtual memory
: The basic idea behind virtual memory is that each program has its own address space, which is broken up into chunks called pages. Each page is a contiguous range of addresses. These pages are mapped onto physical memory, but not all pages have to be in physical memory at the same time to run the program. When the program references a part of its address space that is in physical memory, the hardware performs the necessary mapping on the fly. When the pro- gram references a part of its address space that is not in physical memory, the oper- ating system is alerted to go get the missing piece and re-execute the instruction that failed.

### paging

**MMU (Memory Management Unit)** maps virtual address onto physical memory address.

![virtual](../virtual.png)

A very simple example of how this mapping works is shown in Fig. 3-9. In this example, we have a computer that generates 16-bit addresses, from 0 up to 64K − 1. These are the virtual addresses. This computer, however, has only 32 KB of physical memory. So although 64-KB programs can be written, they cannot be loaded into memory in their entirety and run. A complete copy of a program’s core image, up to 64 KB, must be present on the disk, however, so that pieces can be brought in as needed.
The virtual address space consists of fixed-size units called pages. The corres- ponding units in the physical memory are called page frames. The pages and page frames are generally the same size. In this example they are 4 KB, but page sizes from 512 bytes to a gigabyte have been used in real systems. With 64 KB of virtual address space and 32 KB of physical memory, we get 16 virtual pages and 8 page frames. Transfers between RAM and disk are always in whole pages.
When the program tries to access address 0, for example, using the instruction
`MOV REG,0`
virtual address 0 is sent to the MMU. The MMU sees that this virtual address falls in page 0 (0 to 4095), which according to its mapping is page frame 2 (8192 to 12287). It thus transforms the address to 8192 and outputs address 8192 onto the bus. The memory knows nothing at all about the MMU and just sees a request for reading or writing address 8192, which it honors. Thus, the MMU has effectively mapped all virtual addresses between 0 and 4095 onto physical addresses 8192 to 12287.

this ability to map the 16 virtual pages onto any of the eight page frames by setting the MMU’s map appropriately does not solve the problem that the virtual address space is larger than the physical memory. 
In the actual hardware, a **Present/absent** bit keeps track of which pages are physi- cally present in memory.

**What if program references an unmapped address?**

it's a **page fault** and the OS picks a little-used page frame (we'll cover later how to pick) and write is contents back to the disk (if it's **dirty**, meaning its content is different from what's in disk). It then fetches from disk the page that was just referenced into the pages frame just freed, changes the map, and restarts the trapped instruction.

### page table
page table
: the purpose of the page table is to map virtual pages onto page frames.

![page_table](../page_table.png)

![page_table2](../page_table2.png)

protection bit
: tells what kinds of accesss are permitted, it could be 1 bit (o for read/write and 1 for read only), or it could have 3 bits, each one enabling read, write and execute the page.

modified
: set when the page is written to. aka "dirty bit". A page that is drity must be written back to the disk.

referenced
: set when a page is referenced, either for reading or for writing. Its value help the OS to choose a page to evict when a *page fault* happens.

caching disabled
: allows caching to be disabled for the page. This feature is important for pages that map onto device registers rather than memory. If the oper- ating system is sitting in a tight loop waiting for some I/O device to respond to a command it was just given, it is essential that the hardware keep fetching the word from the device, and not use an old cached copy.

### better understanding page, page table, and sizes
{{<hint info>}}
Note that virtual memory size 64Kb does not mean page table is 64Kb. page table size can be calculated as following [link](https://stackoverflow.com/questions/4029838/determine-page-table-size-for-virtual-memory)

Consider a virtual memory system with a 38-bit virtual byte address, 1KB pages and 512 MB of physical memory. What is the total size of the page table for each process on this machine, assuming that the valid, protection, dirty and use bits take a total of 4 bits, and that all the virtual pages are in use? (assume that disk addresses are not stored in the page table.)
{{</hint>}}

Well, if the question is simply "what is the size of the page table?" irrespective of whether it will fit into physical memory, the answer can be calculated thus:

First physical memory. There are 512K pages of physical memory (512M / 1K). This requires 19 bits to represent each page. Add that to the 4 bits of accounting information and you get 23 bits.

Now virtual memory. With a 38-bit address space and a 10-bit (1K) page size, you need 228 entries in your page table.

Therefore 228 page table entries at 23 bits each is 6,174,015,488 bits or 736M.

That's the maximum size needed for a single-level VM subsystem for each process.

Now obviously that's not going to work if you only have 512M of physical RAM so you have a couple of options.

You can reduce the number of physical pages. For example, only allow half of the memory to be subject to paging, keeping the other half resident at all time. This will save one bit per entry, not really enough to make a difference.

Increase the page size, if possible. A 1K page on a 38-bit address space is the reason for the very chunky page tables. For example, I think the '386, with its 32-bit address space, uses 4K pages. That would result in a million page table entries, far less than the 260 million required here.

Go multi-level. A bit more advanced but it basically means that the page tables themselves are subject to paging. You have to keep the first level of page tables resident in physical memory but the second level can go in and out as needed. This will greatly reduce the physical requirements but at the cost of speed, since two levels of page faults may occur to get at an actual process page (one for the secondary paging tables then one for the process page).

Let's look a little closer at option 3.

If we allow 32M for the primary paging table and give each entry 4 bytes (32 bits: only 23 are needed but we can round up for efficiency here), this will allow 8,388,608 pages for the secondary page table.

Since each of those secondary page table pages is 1K long (allowing us to store 256 secondary page table entries at 4 bytes each), we can address a total of 2,147,483,648 virtual pages.

This would allow 8,192 fully-loaded (i.e., using their entire 28-bit address space) processes to run side by side, assuming you have a fair chunk of disk space to store the non-resident pages.

Now obviously the primary paging table (and the VM subsystem, and probably a fair chunk of the rest of the OS) has to stay resident at all times. You cannot be allowed to page out one of the primary pages since you may well need that page in order to bring it back in :-)

But that's a resident cost of only 32M of the 512M for the primary paging table, much better than the (at a minimum, for one fully-loaded process) of 736M.

## Speeding up paging

Two major issues:
1. mapping from virtual address to physical address must be fast
2. If the virtual address space is large, the page table will be large.

### Translation Lookaside Buffers (TLB)
most programs tend to make a large number of references to a small number of pages, and not the other way around. Thus only a small fraction of the page table entries are heavily read; the rest are barely used at all.

TLB
: aka associative memory. It is usually inside the MMU and consists of a small number of entries (rarely > 256). Each entry contains information about one page, including the virtual page number, a bit that is set when the page is modified, the protection code (read/write/execute permissions), and the physical page frame in which the page is located. These fields have a one-to-one correspondence with the fields in the page table, except for the virtual page number, which is not needed in the page table. Another bit indicates whether the entry is valid (i.e., in use) or not.

When a virtual address is presented to the MMU for translation, the hardware first checks to see if its virtual page number is present in the TLB by comparing it to all the entries simultaneously (i.e., in par- allel). Doing so requires special hardware, which all MMUs with TLBs have. If a valid match is found and the access does not violate the protection bits, the page frame is taken directly from the TLB, without going to the page table. If the virtu- al page number is present in the TLB but the instruction is trying to write on a read-only page, a protection fault is generated.

The interesting case is what happens when the virtual page number is not in the TLB. The MMU detects the miss and does an ordinary page table lookup. It then evicts one of the entries from the TLB and replaces it with the page table entry just looked up. Thus if that page is used again soon, the second time it will result in a TLB hit rather than a miss.

### Software TLB Management
In the past, MMU and TLB are in hardware.
Now, we do page management in software.

When a TLB miss occurs, instead of the MMU going to the page tables to find and fetch the needed page reference, it just generates a TLB fault and tosses the problem into the lap of the operating system. The system must find the page, remove an entry from the TLB, enter the new one, and restart the instruction that faulted. And, of course, all of this must be done in a handful of instructions because TLB misses occur much more frequently than page faults.
Surprisingly enough, if the TLB is moderately large (say, 64 entries) to reduce the miss rate, software management of the TLB turns out to be acceptably efficient. The main gain here is a much simpler MMU, which frees up a considerable amount of area on the CPU chip for caches and other features that can improve performance. 

#### different page fault
Suppose the page walk does not find the page in the process’ page table and the program thus incurs a page fault. There are three possibilities. 
1. the page may actually be in memory, but not in this process’ page table. For instance, the page may have been brought in from disk by another process. In that case, we do not need to access the disk again, but merely map the page appropriately in the page tables. This is **a minor page fault**.
2. **a major page fault** occurs if the page needs to be brought in from disk.
3. The program accessed an invalid address and no mapping needs to be added in the TLB at all. In that case, the operating system typically kills the program with a **segmentation fault**. Only in this case did the program do something wrong. All other cases are automatically fixed by the hardware and/or the operating system — at the cost of some performance.

### Page Tables for large memories

#### multilevel page tables
![multi](../multilevel.png)

#### inverted page tables

## Page Replacement Algorithms
When a page fault occurs, the operating system has to choose a page to evict (remove from memory) to make room for the incoming page. If the page to be removed has been modified while in memory, it must be rewritten to the disk to bring the disk copy up to date. If, however, the page has not been changed (e.g., it con- tains program text), the disk copy is already up to date, so no rewrite is needed. The page to be read in just overwrites the page being evicted.
While it would be possible to pick a random page to evict at each page fault, system performance is much better if a page that is not heavily used is chosen. 

### not recent used

### FIFO

### second-chance

### clock

### LRU (least recently used)

### working set

### wsclock TODO

## Design issues for paging systems TODO
### page size
small page size v.s. large page size
- small page size: less *internal fragmentation*
- small page size: less waite of memory 
- small page size: larger page table
- small page size: use up much valuable space in the TLB (?)


## Segmentation TODO